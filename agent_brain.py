import os
import time
import hashlib
import traceback
import re
import requests
import logging
from typing import Optional, Dict
from dotenv import load_dotenv
from notion_client import Client
from duckduckgo_search import DDGS

from agent_notion import (
    NOTION_TOKEN,
    NOTION_DATABASE_ID,
    validate_env,
    extract_plain_rich_text,
    NotionAgent,
)
from backup_data import backup_notion_data

load_dotenv()

# Telegram Configuration
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

# Configure Logging
class TelegramHandler(logging.Handler):
    """Custom Logging Handler that sends critical logs to Telegram."""
    def emit(self, record):
        if record.levelno >= logging.ERROR:
            try:
                msg = self.format(record)
                send_telegram_message(f"ğŸš¨ <b>System Error</b>\n<pre>{msg}</pre>")
            except Exception:
                self.handleError(record)

# Setup Logger
logger = logging.getLogger("NotionAgent")
logger.setLevel(logging.INFO)

# Console Handler
c_handler = logging.StreamHandler()
c_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(c_handler)

# Telegram Handler
if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
    t_handler = TelegramHandler()
    t_handler.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(t_handler)

def send_telegram_message(message: str) -> None:
    """Send a message to Telegram."""
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logger.warning("Telegram configuration missing. Skipping notification.")
        return

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML"
    }
    try:
        resp = requests.post(url, json=payload, timeout=10)
        if resp.status_code != 200:
            logger.error(f"Telegram send failed: {resp.text}")
    except Exception as e:
        logger.error(f"Telegram connection error: {e}")

def md5_of_text(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def search_for_alternative_url(title: str, original_url: str) -> Optional[str]:
    """
    Search for a new URL if the original one is broken.
    Uses title + 'Github' or 'Cursor Rules' as query.
    """
    query = f"{title} Github Cursor Rules"
    logger.info(f"ğŸ” å°è¯•è‡ªåŠ¨ä¿®å¤é“¾æ¥ï¼Œæœç´¢å…³é”®è¯: {query}")
    
    try:
        results = DDGS().text(query, max_results=3)
        if not results:
            return None
            
        original_domain = original_url.split('/')[2] if '//' in original_url else ""
        
        for res in results:
            href = res.get('href')
            if not href:
                continue
                
            # Smart Matching Logic
            # 1. Same domain match (likely rename/move)
            if original_domain and original_domain in href:
                logger.info(f"âœ¨ å‘ç°åŒåŸŸåæ–°é“¾æ¥: {href}")
                return href
                
            # 2. Trusted Source Match (GitHub/Official)
            if "github.com" in href:
                logger.info(f"âœ¨ å‘ç°å¯ä¿¡æº(GitHub): {href}")
                return href
                
    except Exception as e:
        logger.error(f"Search failed: {e}")
        
    return None

def fetch_remote_text(url: str, timeout: int = 15) -> Optional[str]:
    max_retries = 3
    for attempt in range(max_retries):
        try:
            logger.info(f"è·å–è¿œç¨‹å†…å®¹: {url} (Attempt {attempt+1}/{max_retries})")
            resp = requests.get(url, timeout=timeout)
            if resp.status_code == 200:
                text = resp.text
                if not text:
                    logger.error(f"è¿œç¨‹å†…å®¹ä¸ºç©º: {url}")
                    return None
                return text
            elif resp.status_code == 404:
                logger.warning(f"404 Not Found: {url}")
                return None
            else:
                logger.error(f"è¯·æ±‚å¤±è´¥ {resp.status_code}: {url}")
        except Exception as e:
            logger.error(f"è¯·æ±‚å¼‚å¸¸ {url}: {e}")
        
        if attempt < max_retries - 1:
            time.sleep(2)
            
    return None

def sync_existing_sources(notion_client: Client, agent: NotionAgent) -> Dict[str, int]:
    logger.info("å¼€å§‹å­˜é‡æ›´æ–°ï¼šåŸºäº Source é“¾æ¥å·¡æ£€")
    start_cursor: Optional[str] = None
    stats = {"created": 0, "updated": 0, "skipped": 0, "error": 0}

    while True:
        # Use direct requests to avoid library version issues
        url = f"https://api.notion.com/v1/databases/{NOTION_DATABASE_ID}/query"
        headers = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Notion-Version": "2022-06-28",
            "Content-Type": "application/json"
        }
        
        payload = {"page_size": 100}
        if start_cursor:
            payload["start_cursor"] = start_cursor

        try:
            r = requests.post(url, json=payload, headers=headers, timeout=30)
            if r.status_code != 200:
                logger.error(f"Notion API Error: {r.text}")
                break
            resp = r.json()
        except Exception as e:
            logger.error(f"Notion Request Failed: {e}")
            break

        results = resp.get("results", [])
        if not results:
            break

        for page in results:
            props = page.get("properties", {})
            
            # Extract Title
            name_prop = props.get("Name") or {}
            title_items = name_prop.get("title", [])
            if not title_items:
                continue
            title = title_items[0].get("plain_text") or title_items[0].get("text", {}).get("content", "")
            if not title:
                continue

            # Extract Source URL
            source_prop = props.get("Source")
            source_url = None
            if source_prop and source_prop.get("type") == "url":
                source_url = source_prop.get("url")

            # Extract Tag
            type_prop = props.get("Type")
            tag_name: Optional[str] = None
            if type_prop and type_prop.get("type") == "select":
                sel = type_prop.get("select")
                if sel:
                    tag_name = sel.get("name")
            if tag_name not in ["Skill", "MCP"]:
                tag_name = "Skill"

            # Extract Current Status
            status_prop = props.get("Status")
            current_status = "Active"
            if status_prop:
                current_status = status_prop.get("status", {}).get("name") or "Active"

            # Extract Local Content
            content_prop = props.get("Content") or {}
            local_text = extract_plain_rich_text(content_prop)

            # 1. Self-managed / AI Created (No Source)
            if not source_url:
                if current_status != "Active":
                    logger.info(f"è‡ªå»ºå†…å®¹çŠ¶æ€ä¿®å¤: {title} -> Active")
                    res = agent.save_to_notion(
                        title=title, content=local_text, tag=tag_name, url=None, status="Active"
                    )
                    stats[res] = stats.get(res, 0) + 1
                else:
                    # No source, already active, skip
                    pass
                continue

            # 2. Remote Fetch with Retry
            remote_text = fetch_remote_text(source_url)

            # 3. Dead Link / Fetch Failure
            if remote_text is None:
                # Try Self-Healing
                logger.warning(f"âš ï¸  é“¾æ¥å¤±æ•ˆï¼Œå°è¯•è‡ªæ„ˆ: {title}")
                new_url = search_for_alternative_url(title, source_url)
                
                if new_url:
                    # Healing Success
                    logger.info(f"ğŸš‘ è‡ªæ„ˆæˆåŠŸ: {source_url} -> {new_url}")
                    remote_text_healed = fetch_remote_text(new_url)
                    
                    if remote_text_healed:
                        content_healed = f"è‡ªåŠ¨åŒæ­¥è‡ª Sourceï¼š{new_url}\n\n{remote_text_healed}"
                        res = agent.save_to_notion(
                            title=title, content=content_healed, tag=tag_name, url=new_url, status="Active"
                        )
                        stats[res] = stats.get(res, 0) + 1
                        
                        send_telegram_message(
                            f"ğŸ”— <b>å·²è‡ªåŠ¨ä¿®å¤æ­»é“¾</b>\n"
                            f"ğŸ“ <b>{title}</b>\n"
                            f"âŒ åŸ: {source_url}\n"
                            f"âœ… æ–°: {new_url}"
                        )
                        continue
                
                # Healing Failed
                if current_status != "Broken":
                    logger.error(f"âŒ æ­»é“¾è‡ªæ„ˆå¤±è´¥: {source_url} -> æ ‡è®°ä¸º Broken")
                    # Update status to Broken
                    res = agent.save_to_notion(
                        title=title, content=local_text, tag=tag_name, url=source_url, status="Broken"
                    )
                    stats[res] = stats.get(res, 0) + 1
                else:
                    logger.info(f"âŒ æ­»é“¾ä¿æŒ Broken: {title}")
                    stats["skipped"] += 1
                continue

            # 4. Success - Update Content & Restore Status
            if current_status != "Active":
                logger.info(f"âœ… é“¾æ¥æ¢å¤: {title} -> æ¢å¤ Active")
                # Will be updated in save_to_notion call below

            local_md5 = md5_of_text(local_text)
            remote_md5 = md5_of_text(remote_text)

            if local_md5 == remote_md5 and current_status == "Active":
                logger.info(f"â­ï¸  [MD5 Match] è·³è¿‡æ›´æ–°: {title}")
                stats["skipped"] += 1
                continue

            new_content = f"è‡ªåŠ¨åŒæ­¥è‡ª Sourceï¼š{source_url}\n\n{remote_text}"
            logger.info(f"æ£€æµ‹åˆ°ä¸Šæ¸¸å˜æ›´æˆ–çŠ¶æ€ä¿®å¤: {title}")

            res = agent.save_to_notion(
                title=title,
                content=new_content,
                tag=tag_name,
                url=source_url,
                status="Active",
            )
            stats[res] = stats.get(res, 0) + 1

        if not resp.get("has_more"):
            break

        start_cursor = resp.get("next_cursor")

    logger.info("å­˜é‡æ›´æ–°å®Œæˆ")
    return stats

def discover_new_rules(agent: NotionAgent) -> Dict[str, int]:
    logger.info("å¼€å§‹å¢é‡å‘ç°ï¼šStripe & Automation è§„åˆ™")
    stats = {"created": 0, "updated": 0, "skipped": 0, "error": 0}
    
    keywords = ["stripe", "automation"]
    base = "https://cursor.directory"
    seen_titles = set()

    for kw in keywords:
        search_url = f"https://cursor.directory/search?q={kw}"
        logger.info(f"æ­£åœ¨æœç´¢ cursor.directory: {kw}")
        html = fetch_remote_text(search_url)
        if not html:
            continue

        # Regex to find rule links. 
        # Adapting to potential structure: <a href="/rules/foo-bar"> ... <h3>Foo Bar</h3> ... </a>
        # This is a best-effort regex based on common patterns.
        pattern = re.compile(r'href=\"(/rules/[^\"]+)\"', re.IGNORECASE)
        matches = pattern.findall(html)
        
        if not matches:
            logger.info(f"æœªæ‰¾åˆ°å…³äº {kw} çš„è§„åˆ™")
            continue

        for path in matches:
            # Deduplicate by URL path
            if path in seen_titles:
                continue
            seen_titles.add(path)

            # Derive title from path
            # /rules/stripe-api-best-practices -> Stripe Api Best Practices
            title = path.replace("/rules/", "").replace("-", " ").title()
            
            rule_url = base + path
            remote_text = fetch_remote_text(rule_url)
            if not remote_text:
                continue

            content = (
                f"ä¸­æ–‡åŠŸèƒ½ç®€ä»‹ï¼šè‡ªåŠ¨å‘ç°çš„ {kw} è§„åˆ™ [{title}]ï¼Œ"
                f"ç”¨äºå¢å¼º Agent åœ¨è¯¥é¢†åŸŸçš„è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚\n\n"
                f"Original Content ({rule_url}):\n\n"
                f"{remote_text}"
            )

            logger.info(f"å¢é‡å‘ç°ï¼šå°è¯•å…¥åº“æ–°è§„åˆ™: {title}")
            res = agent.save_to_notion(
                title=title,
                content=content,
                tag="Skill",
                url=rule_url,
                status="Active",
            )
            stats[res] = stats.get(res, 0) + 1
            
            # Be polite to the server
            time.sleep(1)

    logger.info("å¢é‡å‘ç°æµç¨‹å®Œæˆ")
    return stats

def run_once() -> None:
    validate_env()
    agent = NotionAgent()
    client = Client(auth=NOTION_TOKEN)

    logger.info("å¼€å§‹æœ¬è½®å·¡æ£€ï¼šå­˜é‡æ›´æ–° + å¢é‡å‘ç°")
    
    # Run tasks and aggregate stats
    s1 = sync_existing_sources(client, agent)
    s2 = discover_new_rules(agent)
    
    total_created = s1["created"] + s2["created"]
    total_updated = s1["updated"] + s2["updated"]
    total_skipped = s1["skipped"] + s2["skipped"]
    
    report_msg = (
        f"âœ… æ–°å¢: {total_created} | ğŸ”„ æ›´æ–°: {total_updated} | â­ï¸ è·³è¿‡: {total_skipped}"
    )
    logger.info(report_msg)
    
    # Run Backup (Once per cycle, effectively daily in current loop)
    try:
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ•°æ®å†·å¤‡ä»½...")
        backup_notion_data(client, NOTION_DATABASE_ID)
    except Exception as e:
        logger.error(f"Backup failed: {e}")
        send_telegram_message(f"âš ï¸ <b>å¤‡ä»½å¤±è´¥</b>\n{str(e)}")
    
    # Send Telegram Report
    send_telegram_message(f"<b>å·¡æ£€æŠ¥å‘Š</b>\n{report_msg}")

def main() -> None:
    validate_env()
    logger.info("Agent Brain initialized (7x24h auto-inspection mode)")
    
    while True:
        start = time.time()
        try:
            run_once()
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œé€€å‡ºå·¡æ£€")
            break
        except Exception as e:
            error_msg = f"è„šæœ¬å·¡æ£€æŠ¥é”™: {str(e)}"
            logger.error(error_msg)
            traceback.print_exc()
            send_telegram_message(f"ğŸš¨ <b>ç´§æ€¥é¢„è­¦</b>\n{error_msg}")

        elapsed = time.time() - start
        sleep_seconds = max(0, 24 * 60 * 60 - int(elapsed))
        logger.info(f"ä¸‹æ¬¡å·¡æ£€å°†åœ¨ {sleep_seconds} ç§’åæ‰§è¡Œ")
        time.sleep(sleep_seconds)

if __name__ == "__main__":
    main()
